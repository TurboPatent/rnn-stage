<p>
  <span data-role="tag" data-value="524b96ce-de49-6a12-4bcc-3ade5c5e29b0" id="id38925949-4abe-b924-b28d-d7e56404f9a3" data-type="figure">Figure 3</span>&nbsp;illustrates a RNN architecture with&nbsp;
  <span data-role="tag" data-value="907af07d-0332-710b-af1c-53d685b2e5cd" id="id95d19348-54df-9fe4-3115-d13b1d235279" data-type="drawingObject">long short-term memory 300</span>&nbsp;(LSTM).
</p>
<p>All RNNs have the form of a chain of repeating nodes, each node being a&nbsp;neural network. In standard RNNs, this repeating node will have a structure&nbsp;such as a single layer with a tanh activation function. This is shown in the upper diagram. An LSTMs also has this chain like design, but the repeating node A has a different structure than for regular RNNs. Instead of having a single neural network layer, there are typically four, and the layers interact in a particular way.&nbsp;</p>
<p>In an LSTM&nbsp;each path carries an entire vector, from the output of one node to the inputs of others. The circled functions outside the dotted box&nbsp;represent pointwise operations, like vector addition, while the sigmoid and tanh&nbsp;boxes inside the dotted box are learned neural network layers. Lines merging denote concatenation, while a line forking denote values being copied and the copies going to different locations.</p>
<p>An important feature of LSTMs is the cell state Ct, the horizontal line running through the top of the 
  <span data-role="tag" data-value="907af07d-0332-710b-af1c-53d685b2e5cd" id="id745f2fe7-0ef6-4c28-79b6-d5e93e3593d2" data-type="drawingObject">long short-term memory 300</span>&nbsp;(lower diagram). The cell state is like a conveyor belt. It runs across the entire chain, with only some minor linear interactions. It’s entirely possible for signals to flow along it unchanged. The LSTM has the ability to remove or add information to the cell state, carefully regulated by structures called gates. Gates are a way to optionally let information through a cell. They are typically formed using a sigmoid neural net layer and a pointwise multiplication operation.
</p>
<p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through”. An LSTM has three of these sigmoid gates, to protect and control the cell state.</p>