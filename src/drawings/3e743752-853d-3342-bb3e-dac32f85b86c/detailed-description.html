<p>
  <span data-role="tag" data-value="3e743752-853d-3342-bb3e-dac32f85b86c" id="id3844bf93-2dbc-f4ca-294c-90f2be9fbd96" data-type="figure">Figure 1</span>&nbsp;illustrates a 
  <span data-role="tag" data-value="7bd5b7aa-7127-0e53-e83d-1b959bb84a2c" id="id9c5d0474-a8f0-ab26-066d-449e8b22f7c6" data-type="drawingObject">recurrent neural network 100</span>&nbsp;(RNN). Variable&nbsp;x[t] is the input at stage t. For example, x[1] could be a&nbsp;one-hot vector corresponding to&nbsp;the second word of a sentence. Variable&nbsp;s[t] is the hidden state at stage t. It’s the “memory” of the network. The variable s[t]&nbsp;is calculated based on the previous hidden state and the input at the current stage:&nbsp;&nbsp;s[t]=f(Ux[t] + Ws[t-1]).&nbsp;The activation function f usually&nbsp;is a nonlinearity such as&nbsp;tanh&nbsp;or&nbsp;ReLU. The input s(-1) , which is required to calculate the first hidden state, is typically initialized to all zeroes. Variable&nbsp;o[t] is the output at stage t. For example, to predict the next word in a sentence it would&nbsp;be a vector of probabilities across the vocabulary: o[t]=softmax(Vs[t]).
</p>