<p>
  <span data-role="tag" data-value="160d6785-856f-0361-2f24-220a90588592" id="iddd6d48f5-461d-45c5-5be2-771227e49588" data-type="figure">Figure 2</span>&nbsp;illustrates a 
  <span data-role="tag" data-value="721b6bb7-1985-e6f5-1ab6-c9203634debc" id="idb9278d03-fd26-9689-1b74-9b9ea1241f2e" data-type="drawingObject">bidirectional recurrent neural network 200</span>&nbsp;(BRNN). BRNNs&nbsp;are designed for situation where&nbsp;the output at a stage may not only depend on the previous inputs in the sequence, but also future elements. For example, to predict a missing word in a sequence a BRNN will consider both the&nbsp;left and the right context. BRNNs may be implemented as two RNNs in which the output Y is computed based on the hidden states S of both RNNs and the inputs X. In the 
  <span data-role="tag" data-value="721b6bb7-1985-e6f5-1ab6-c9203634debc" id="idbf6d1f7e-64bf-d3de-ba20-0d4d9231ba05" data-type="drawingObject">bidirectional recurrent neural network 200</span>&nbsp;shown in&nbsp;
  <span data-role="tag" data-value="160d6785-856f-0361-2f24-220a90588592" id="id4f7387fa-65cf-732a-0593-1b4539c5ac1f" data-type="figure">Figure 2</span>, each node A is typically itself a neural network.&nbsp;&nbsp;Deep BRNNs&nbsp;are similar to BRNNs, but have multiple layers per node A. In practice this enables a higher learning capacity but also requires more training data than for single layer networks.&nbsp;
</p>