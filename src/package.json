{
  "name": "rnn-stage",
  "description": "Deep Learning: Recurrent Neural Network",
  "owner": {
    "login": "TurboPatent",
    "type": "organization"
  },
  "license": "MIT",
  "isPrivate": false,
  "details": {
    "counts": {
      "termCount": 0,
      "claimCount": 1,
      "independentCount": 1,
      "drawingCount": 3,
      "glossaryCount": 5
    },
    "briefDescriptions": [
      "<p>illustrates a <span data-role=\"tag\" data-value=\"7bd5b7aa-7127-0e53-e83d-1b959bb84a2c\" id=\"id90ecd117-8ac0-ce45-66ef-f0bb736bc2cf\" data-type=\"drawingObject\">recurrent neural network 100</span> in accordance with one embodiment.</p>",
      "<p>illustrates a <span data-role=\"tag\" data-value=\"721b6bb7-1985-e6f5-1ab6-c9203634debc\" id=\"idc24af75a-39a4-84e6-e133-e571f093c1c4\" data-type=\"drawingObject\">bidirectional recurrent neural network 200</span> in accordance with one embodiment.</p>",
      "<p>illustrates a <span data-role=\"tag\" data-value=\"907af07d-0332-710b-af1c-53d685b2e5cd\" id=\"id20ce07ce-dd0e-ad48-3d97-839a32774c6b\" data-type=\"drawingObject\">long short-term memory 300</span> in accordance with one embodiment.</p>"
    ],
    "parts": [
      {
        "name": "recurrent neural network",
        "number": 100
      },
      {
        "name": "bidirectional recurrent neural network",
        "number": 200
      },
      {
        "name": "long short-term memory",
        "number": 300
      }
    ],
    "terms": [],
    "glossary": [
      {
        "term": "CTC loss function",
        "definition": "connectionist temporal classification, a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task. Fortunately there is an efficient forward–backward algorithm for that. CTC scores can then be used with the back-propagation algorithm to update the neural network weights. Alternative approaches to a CTC-fitted neural network include a hidden Markov model (HMM)."
      },
      {
        "term": "dequeue max finder",
        "definition": "[[need a definition]]"
      },
      {
        "term": "Gated Recurrent Unit (GRU)",
        "definition": "are a gating mechanism in recurrent neural networks. GRUs may exhibit better performance on smaller datasets than do LSTMs. They have fewer parameters than LSTM, as they lack an output gate.\nSee https://en.wikipedia.org/wiki/Gated_recurrent_unit"
      },
      {
        "term": "beam search",
        "definition": "a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates.[1] It is thus a greedy algorithm.\nBeam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost.[2] However, it only stores a predetermined number, β, of best states at each level (called the beam width). Only those states are expanded next. The greater the beam width, the fewer states are pruned. With an infinite beam width, no states are pruned and beam search is identical to breadth-first search. The beam width bounds the memory required to perform the search. Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists). Beam search is not optimal (that is, there is no guarantee that it will find the best solution).\nIn general, beam search returns the first solution found. Beam search for machine translation is a different case: once reaching the configured maximum search depth (i.e. translation length), the algorithm will evaluate the solutions found during search at various depths and return the best one (the one with the highest probability).\nThe beam width can either be fixed or variable. One approach that uses a variable beam width starts with the width at a minimum. If no solution is found, the beam is widened and the procedure is repeated."
      },
      {
        "term": "Adam optimizer",
        "definition": "an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. Adam as combining the advantages of two other extensions of stochastic gradient descent. Specifically, Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems), and Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\nAdam realizes the benefits of both AdaGrad and RMSProp.\nInstead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).\nSpecifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.\nThe initial value of the moving averages and beta1 and beta2 values close to 1.0 (recommended) result in a bias of moment estimates towards zero. This bias is overcome by first calculating the biased estimates before then calculating bias-corrected estimates."
      }
    ]
  },
  "repository": "https://github.com/TurboPatent/rnn-stage"
}